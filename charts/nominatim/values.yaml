## @section Global parameters
## Global Docker image parameters
## Please, note that this will override the image parameters, including dependencies, configured to use the global value
## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
##

## @param global.imageRegistry Global Docker image registry
## @param global.imagePullSecrets Global Docker registry secret names as an array
## @param global.storageClass Global StorageClass for Persistent Volume(s)
##
global:
  security:
    allowInsecureImages: true #required for bitnamilegacy
  imageRegistry: ""
  ## E.g.
  ## imagePullSecrets:
  ##   - myRegistryKeySecretName
  ##
  imagePullSecrets: []
  storageClass: ""

## @section Common parameters
##

## @param kubeVersion Override Kubernetes version
##
kubeVersion: ""
## @param nameOverride String to partially override common.names.fullname template (will maintain the release name)
##
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname template
##
fullnameOverride: ""
## @param commonLabels Labels to add to all deployed resources
##
commonLabels: {}
## @param commonAnnotations Annotations to add to all deployed resources
##
commonAnnotations: {}
## @param clusterDomain Kubernetes Cluster Domain
##
clusterDomain: cluster.local
## @param extraDeploy Array of extra objects to deploy with the release
##
extraDeploy: []

## Enable diagnostic mode in the deployment
##
diagnosticMode:
  ## @param diagnosticMode.enabled Enable diagnostic mode (all probes will be disabled and the command will be overridden)
  ##
  enabled: false
  ## @param diagnosticMode.command Command to override all containers in the deployment
  ##
  command:
    - sleep
  ## @param diagnosticMode.args Args to override all containers in the deployment
  ##
  args:
    - infinity

## @section Nominatirm Image parameters
##

## Nominatim image
## ref: https://hub.docker.com/r/mediagis/nominatim/tags
## @param image.registry Nominatim image registry
## @param image.repository Nominatim image repository
## @param image.tag Nominatim image tag (immutable tags are recommended)
## @param image.digest Nominatim image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
## @param image.pullPolicy Nominatim image pull policy
## @param image.pullSecrets Nominatim image pull secrets
## @param image.debug Specify if debug values should be set
##
image:
  registry: docker.io
  repository: mediagis/nominatim
  tag: 5.1.0
  digest: ""
  ## Specify a imagePullPolicy
  ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
  ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
  ##
  pullPolicy: IfNotPresent
  ## Optionally specify an array of imagePullSecrets.
  ## Secrets must be manually created in the namespace.
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
  ## e.g:
  ## pullSecrets:
  ##   - myRegistryKeySecretName
  ##
  pullSecrets: [ ]
  ## Enable debug mode
  ##
  debug: false


## @param command Override default container command (useful when using custom images)
##
command:
  - /bin/bash
  - -ec
  - |
    gunicorn \
      --bind :8080 \
      --workers 4 \
      --enable-stdio-inheritance \
      --worker-class uvicorn.workers.UvicornWorker \
      nominatim_api.server.falcon.server:run_wsgi

## @param args Override default container args (useful when using custom images)
##
args: []
## @param extraEnvVars Array with extra environment variables to add to the Nominatim container
## e.g:
## extraEnvVars:
##   - name: memory_limit
##     value: "256M"
##
extraEnvVars: []
## @param extraEnvVarsCM Name of existing ConfigMap containing extra env vars
##
extraEnvVarsCM: ""
## @param extraEnvVarsSecret Name of existing Secret containing extra env vars
##
extraEnvVarsSecret: ""

## @section Nominatim deployment parameters
##

## @param replicaCount Number of Nominatim replicas to deploy
## NOTE: ReadWriteMany PVC(s) are required if replicaCount > 1
##
replicaCount: 1
## @param revisionHistoryLimit Number of old ReplicaSets to retain to allow rollback
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#revision-history-limit
revisionHistoryLimit: 10
## @param updateStrategy.type Nominatim deployment strategy type
## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
## NOTE: Set it to `Recreate` if you use a PV that cannot be mounted on multiple pods
## e.g:
## updateStrategy:
##  type: RollingUpdate
##  rollingUpdate:
##    maxSurge: 25%
##    maxUnavailable: 25%
##
updateStrategy:
  type: RollingUpdate
## @param schedulerName Alternate scheduler
## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
##
schedulerName: ""
## @param terminationGracePeriodSeconds In seconds, time given to the Nominatim pod to terminate gracefully
## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods
##
terminationGracePeriodSeconds: ""
## @param topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
##
topologySpreadConstraints: []
## @param priorityClassName Name of the existing priority class to be used by Nominatim pods, priority class needs to be created beforehand
## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
##
priorityClassName: ""
## @param hostAliases [array] Nominatim pod host aliases
## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
##
hostAliases:
  ## Required for Apache exporter to work
  ##
  - ip: "127.0.0.1"
    hostnames:
      - "status.localhost"
## @param extraVolumes Optionally specify extra list of additional volumes for Nominatim pods
##
extraVolumes: []
## @param extraVolumeMounts Optionally specify extra list of additional volumeMounts for Nominatim container(s)
##
extraVolumeMounts: []
## @param sidecars Add additional sidecar containers to the Nominatim pod
## e.g:
## sidecars:
##   - name: your-image-name
##     image: your-image
##     imagePullPolicy: Always
##     ports:
##       - name: portname
##         containerPort: 1234
##
sidecars: []
## @param initContainers Add additional init containers to the Nominatim pods
## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
## e.g:
## initContainers:
##  - name: your-image-name
##    image: your-image
##    imagePullPolicy: Always
##    command: ['sh', '-c', 'do magic here']
##
initContainers: []
## @param podLabels Extra labels for Nominatim pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
##
podLabels: {}
## @param podAnnotations Annotations for Nominatim pods
## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
##
podAnnotations: {}
## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
##
podAffinityPreset: ""
## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
##
podAntiAffinityPreset: soft
## Node affinity preset
## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
##
nodeAffinityPreset:
  ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ##
  type: ""
  ## @param nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set
  ##
  key: ""
  ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set
  ## E.g.
  ## values:
  ##   - e2e-az1
  ##   - e2e-az2
  ##
  values: []
## @param affinity Affinity for pod assignment
## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## NOTE: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
##
affinity: {}
## @param nodeSelector Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
##
nodeSelector: {}
## @param tolerations Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
##
tolerations: []
## Nominatim containers' resource requests and limits
## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
## @param resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
##
resourcesPreset: "micro"
## @param resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
## Example:
## resources:
##   requests:
##     cpu: 2
##     memory: 512Mi
##   limits:
##     cpu: 3
##     memory: 1024Mi
##
resources: {}
## @param extraContainerPorts Optionally specify extra list of additional ports for Nominatim container(s)
## e.g:
## extraContainerPorts:
##   - name: myservice
##     containerPort: 9090
##
extraContainerPorts: []
## Configure Pods Security Context
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
## @param podSecurityContext.enabled Enabled Nominatim pods' Security Context
## @param podSecurityContext.fsGroup Set Nominatim pod's Security Context fsGroup
## @param podSecurityContext.seccompProfile.type Set Nominatim container's Security Context seccomp profile
##
podSecurityContext:
  enabled: false
  fsGroup: 101
  seccompProfile:
    type: "RuntimeDefault"
## Configure Container Security Context (only main container)
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
## @param containerSecurityContext.enabled Enabled Nominatim containers' Security Context
## @param containerSecurityContext.runAsUser Set Nominatim container's Security Context runAsUser
## @param containerSecurityContext.runAsNonRoot Set Nominatim container's Security Context runAsNonRoot
## @param containerSecurityContext.allowPrivilegeEscalation Set Nominatim container's privilege escalation
## @param containerSecurityContext.capabilities.drop Set Nominatim container's Security Context runAsNonRoot
##
containerSecurityContext:
  enabled: false
  runAsUser: 1001
  runAsNonRoot: true
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
## Configure extra options for Nominatim containers' liveness, readiness and startup probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes
## @param livenessProbe.enabled Enable livenessProbe on Nominatim containers
## @skip livenessProbe.httpGet
## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
## @param livenessProbe.periodSeconds Period seconds for livenessProbe
## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
## @param livenessProbe.successThreshold Success threshold for livenessProbe
##
livenessProbe:
  enabled: true
  httpGet:
    path: /status
    port: http
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders:
      - name: Host
        value: localhost
  initialDelaySeconds: 120
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param readinessProbe.enabled Enable readinessProbe on Nominatim containers
## @skip readinessProbe.httpGet
## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
## @param readinessProbe.periodSeconds Period seconds for readinessProbe
## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
## @param readinessProbe.successThreshold Success threshold for readinessProbe
##
readinessProbe:
  enabled: true
  httpGet:
    path: /status
    port: http
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders:
      - name: Host
        value: localhost
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param startupProbe.enabled Enable startupProbe on Nominatim containers
## @skip startupProbe.httpGet
## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
## @param startupProbe.periodSeconds Period seconds for startupProbe
## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
## @param startupProbe.failureThreshold Failure threshold for startupProbe
## @param startupProbe.successThreshold Success threshold for startupProbe
##
startupProbe:
  enabled: false
  httpGet:
    path: /status
    port: http
    ## If using an HTTPS-terminating load-balancer, the probes may need to behave
    ## like the balancer to prevent HTTP 302 responses. According to the Kubernetes
    ## docs, 302 should be considered "successful", but this issue on GitHub
    ## (https://github.com/kubernetes/kubernetes/issues/47893) shows that it isn't.
    ## E.g.
    ## httpHeaders:
    ## - name: X-Forwarded-Proto
    ##   value: https
    ##
    httpHeaders:
      - name: Host
        value: localhost
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## @param customLivenessProbe Custom livenessProbe that overrides the default one
##
customLivenessProbe: {}
## @param customReadinessProbe Custom readinessProbe that overrides the default one
##
customReadinessProbe: {}
## @param customStartupProbe Custom startupProbe that overrides the default one
##
customStartupProbe: {}
## @param lifecycleHooks for the Nominatim container(s) to automate configuration before or after startup
##
lifecycleHooks: {}

## @section Traffic Exposure Parameters
##

## Nominatim service parameters
##
service:
  ## @param service.type Nominatim service type
  ##
  type: LoadBalancer
  ## @param service.ports.http Nominatim service HTTP port
  ##
  ports:
    http: 80
  ## Node ports to expose
  ## @param service.nodePorts.http Node port for HTTP
  ## NOTE: choose port between <30000-32767>
  ##
  nodePorts:
    http: ""
  ## @param service.sessionAffinity Control where client requests go, to the same pod or round-robin
  ## Values: ClientIP or None
  ## ref: https://kubernetes.io/docs/user-guide/services/
  ##
  sessionAffinity: None
  ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
  ## sessionAffinityConfig:
  ##   clientIP:
  ##     timeoutSeconds: 300
  ##
  sessionAffinityConfig: {}
  ## @param service.clusterIP Nominatim service Cluster IP
  ## e.g.:
  ## clusterIP: None
  ##
  clusterIP: ""
  ## @param service.loadBalancerIP Nominatim service Load Balancer IP
  ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-loadbalancer
  ##
  loadBalancerIP: ""
  ## @param service.loadBalancerSourceRanges Nominatim service Load Balancer sources
  ## ref: https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/#restrict-access-for-loadbalancer-service
  ## e.g:
  ## loadBalancerSourceRanges:
  ##   - 10.10.10.0/24
  ##
  loadBalancerSourceRanges: []
  ## @param service.externalTrafficPolicy Nominatim service external traffic policy
  ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
  ##
  externalTrafficPolicy: Cluster
  ## @param service.annotations Additional custom annotations for Nominatim service
  ##
  annotations: {}
  ## @param service.extraPorts Extra port to expose on Nominatim service
  ##
  extraPorts: []
## Configure the ingress resource that allows you to access the Nominatim installation
## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
##
ingress:
  ## @param ingress.enabled Enable ingress record generation for Nominatim
  ##
  enabled: false
  ## @param ingress.pathType Ingress path type
  ##
  pathType: ImplementationSpecific
  ## @param ingress.apiVersion Force Ingress API version (automatically detected if not set)
  ##
  apiVersion: ""
  ## @param ingress.ingressClassName IngressClass that will be be used to implement the Ingress (Kubernetes 1.18+)
  ## This is supported in Kubernetes 1.18+ and required if you have more than one IngressClass marked as the default for your cluster .
  ## ref: https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/
  ##
  ingressClassName: ""
  ## @param ingress.hostname Default host for the ingress record. The hostname is templated and thus can contain other variable references.
  ##
  hostname: nominatim.local
  ## @param ingress.path Default path for the ingress record
  ## NOTE: You may need to set this to '/*' in order to use this with ALB ingress controllers
  ##
  path: /
  ## @param ingress.annotations Additional annotations for the Ingress resource. To enable certificate autogeneration, place here your cert-manager annotations.
  ## For a full list of possible ingress annotations, please see
  ## ref: https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md
  ## Use this parameter to set the required annotations for cert-manager, see
  ## ref: https://cert-manager.io/docs/usage/ingress/#supported-annotations
  ##
  ## e.g:
  ## annotations:
  ##   kubernetes.io/ingress.class: nginx
  ##   cert-manager.io/cluster-issuer: cluster-issuer-name
  ##
  annotations: {}
  ## @param ingress.tls Enable TLS configuration for the host defined at `ingress.hostname` parameter
  ## TLS certificates will be retrieved from a TLS secret with name: `{{- printf "%s-tls" .Values.ingress.hostname }}`
  ## You can:
  ##   - Use the `ingress.secrets` parameter to create this TLS secret
  ##   - Rely on cert-manager to create it by setting the corresponding annotations
  ##   - Rely on Helm to create self-signed certificates by setting `ingress.selfSigned=true`
  ##
  tls: false
  ## @param ingress.selfSigned Create a TLS secret for this ingress record using self-signed certificates generated by Helm
  ##
  selfSigned: false
  ## @param ingress.extraHosts An array with additional hostname(s) to be covered with the ingress record. The host names are templated and thus can contain other variable references.
  ## e.g:
  ## extraHosts:
  ##   - name: kimai.local
  ##     path: /
  ##
  extraHosts: []
  ## @param ingress.extraPaths An array with additional arbitrary paths that may need to be added to the ingress under the main host
  ## e.g:
  ## extraPaths:
  ## - path: /*
  ##   backend:
  ##     serviceName: ssl-redirect
  ##     servicePort: use-annotation
  ##
  extraPaths: []
  ## @param ingress.extraTls TLS configuration for additional hostname(s) to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  ## e.g:
  ## extraTls:
  ## - hosts:
  ##     - nominatim.local
  ##   secretName: nominatim.local-tls
  ##
  extraTls: []
  ## @param ingress.secrets Custom TLS certificates as secrets
  ## NOTE: 'key' and 'certificate' are expected in PEM format
  ## NOTE: 'name' should line up with a 'secretName' set further up
  ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
  ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
  ## It is also possible to create and manage the certificates outside of this helm chart
  ## Please see README.md for more information
  ## e.g:
  ## secrets:
  ##   - name: nominatim.local-tls
  ##     key: |-
  ##       -----BEGIN RSA PRIVATE KEY-----
  ##       ...
  ##       -----END RSA PRIVATE KEY-----
  ##     certificate: |-
  ##       -----BEGIN CERTIFICATE-----
  ##       ...
  ##       -----END CERTIFICATE-----
  ##
  secrets: []
  ## @param ingress.extraRules Additional rules to be covered with this ingress record
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#ingress-rules
  ## e.g:
  ## extraRules:
  ## - host: nominatim.local
  ##     http:
  ##       path: /
  ##       backend:
  ##         service:
  ##           name: nominatim-svc
  ##           port:
  ##             name: http
  ##
  extraRules: []

initJob:
  enabled: false
  initContainers:
    waitForDB:
      image: postgres:17-alpine
    download:
      image: curlimages/curl

  pbfUrl: https://download.geofabrik.de/europe/germany/sachsen-latest.osm.pbf
  importWikipedia: false
  importGB_Postcode: false
  importUS_Postcode: false
  importStyle: full
  # customStyleUrl: https://raw.githubusercontent.com/david-mart/Nominatim/master/settings/import-street.style
  threads: 16
  freeze: false
  wikipediaUrl: https://nominatim.org/data/wikimedia-importance.sql.gz

  ## @param initJob.continue Nominatim import CLI continue arg
  ## If undefined (default) `--continue` flag is not set on nominatim import
  ## If defined, `nominatim import --continue <step>` is set
  ## Supported steps: `[ import-from-file load-data | indexing | db-postprocess ]`
  ## More info on: https://nominatim.org/release-docs/latest/admin/Faq/#can-a-stoppedkilled-import-process-be-resumed
  ##
  continue:

  ## @param initJob.dbWaitTimeout The number of seconds to wait for the DB to be ready to accept connections
  dbWaitTimeout: 300
  ## @param initJob.backoffLimit set backoff limit of the job
  ##
  backoffLimit: 0
  ## @param initJob.annotations [object] Add annotations to the job
  ##
  annotations: {}
  ## @param initJob.podLabels Additional pod labels
  ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param initJob.podAnnotations Additional pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}

  persistence:
    ## @param initJob.persistence.enabled Enable using Persistent Volume Claims for the data volume used in initJob
    ##
    enabled: false
    ## @param initJob.persistence.storageClass Persistent Volume storage class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
    ##
    storageClass:
    ## @param initJob.persistence.accessModes [array] Persistent Volume access modes
    ##
    accessModes:
      - ReadWriteOnce
    ## @param initJob.persistence.size Persistent Volume size
    ##
    size: 100Gi
    ## @param initJob.persistence.dataSource Custom PVC data source
    ##
    dataSource: { }
    ## @param initJob.persistence.existingClaim The name of an existing PVC to use for persistence
    ##
    existingClaim: ""
    ## @param initJob.persistence.selector Selector to match an existing Persistent Volume for Nominatim data PVC
    ## If set, the PVC can't have a PV dynamically provisioned for it
    ## E.g.
    ## selector:
    ##   matchLabels:
    ##     app: my-app
    ##
    selector: { }
    ## @param initJob.persistence.annotations Persistent Volume Claim annotations
    ##
    annotations: { }
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param initJob.podSecurityContext.enabled Enabled Nominatim Replications pods' Security Context
  ## @param initJob.podSecurityContext.fsGroup Set Nominatim Replications pod's Security Context fsGroup
  ## @param initJob.podSecurityContext.seccompProfile.type Set Nominatim Replications container's Security Context seccomp profile
  ##
  podSecurityContext:
    enabled: true
    fsGroup: 101
    seccompProfile:
      type: "RuntimeDefault"
  ## Configure Container Security Context (only main container)
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param initJob.containerSecurityContext.enabled Enabled Nominatim Replications containers' Security Context
  ## @param initJob.containerSecurityContext.runAsUser Set Nominatim Replications container's Security Context runAsUser
  ## @param initJob.containerSecurityContext.runAsNonRoot Set Nominatim Replications container's Security Context runAsNonRoot
  ## @param initJob.containerSecurityContext.allowPrivilegeEscalation Set Nominatim Replications container's privilege escalation
  ## @param initJob.containerSecurityContext.capabilities.drop Set Nominatim Replications container's Security Context runAsNonRoot
  ##
  containerSecurityContext:
    enabled: false
    runAsUser: 1001
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: [ "ALL" ]
  ## @param initJob.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if resources is set (resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "micro"
  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #    requests:
  #      cpu: 4000m
  #      memory: 4Gi
  #    limits:
  #      cpu: 4000m
  #      memory: 4Gi

  ## K8s CronJob configuration
  ## ref: https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/
  ##
updates:
  ## @param updates.enabled How often should the job run (default to every hour)
  ##
  enabled: false
  ## @param updates.enabled How often should the job run (default to every hour)
  ##
  schedule: "0 * * * *"
  ## @param updates.timeZone The time zone name for the given schedule, see <https://en.wikipedia.org/wiki/List_of_tz_database_time_zones>
  ##
  timeZone: ""
  ## @param updates.startingDeadlineSeconds Optional deadline in seconds for starting the job if it misses scheduled time for any reason
  ##
  startingDeadlineSeconds: ""
  ## @param updates.concurrencyPolicy Specifies how to treat concurrent executions of a Job
  ##
  concurrencyPolicy: Forbid
  ## @param updates.suspend This flag tells the controller to suspend subsequent executions
  ##
  suspend: ""
  ## @param updates.successfulJobsHistoryLimit The number of successful finished jobs to retain
  ##
  successfulJobsHistoryLimit: ""
  ## @param updates.failedJobsHistoryLimit The number of failed finished jobs to retain
  ##
  failedJobsHistoryLimit: ""
  ## @param updates.backoffLimit The number of retries before marking this job failed
  ##
  backoffLimit: ""
  ## @param updates.ttlSecondsAfterFinished The maximum retention before removing the job
  ##
  ttlSecondsAfterFinished: ""

  replicationUrl: https://download.geofabrik.de/europe/germany/sachsen-updates/
  ## @param updates.command Override default container command (useful when using custom images)
  ##
  command:
    - /bin/bash
    - -ec
  ## @param updates.args Override default container args (useful when using custom images)
  ## ref: https://nominatim.org/release-docs/latest/admin/Update/#catch-up-mode
  ##
  args:
    - |
      NOMINATIM_REPLICATION_MAX_DIFF=5000 nominatim replication --catch-up --threads 1

  ## @param initJob.podLabels Additional pod labels
  ## Ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  ##
  podLabels: {}
  ## @param initJob.podAnnotations Additional pod annotations
  ## ref: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  ##
  podAnnotations: {}
  ## @param initJob.podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: "hard"
  ## @param initJob.podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAntiAffinityPreset: ""
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param initJob.nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param initJob.nodeAffinityPreset.key Node label key to match. Ignored if `affinity` is set
    ##
    key: ""
    ## @param initJob.nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param initJob.affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## NOTE: podAffinityPreset, podAntiAffinityPreset, and nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param initJob.nodeSelector Node labels for pod assignment
  ## ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param initJob.tolerations Tolerations for pod assignment
  ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param initJob.schedulerName Alternate scheduler
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param initJob.terminationGracePeriodSeconds In seconds, time given to the Nominatim pod to terminate gracefully
  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods
  ##
  terminationGracePeriodSeconds: ""
  ## @param initJob.topologySpreadConstraints Topology Spread Constraints for pod assignment spread across your cluster among failure-domains. Evaluated as a template
  ## Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/#spread-constraints-for-pods
  ##
  topologySpreadConstraints: []
  ## @param initJob.priorityClassName Name of the existing priority class to be used by Nominatim pods, priority class needs to be created beforehand
  ## Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  ##
  priorityClassName: ""
  ## Configure Pods Security Context
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod
  ## @param updates.podSecurityContext.enabled Enabled Nominatim Replications pods' Security Context
  ## @param updates.podSecurityContext.fsGroup Set Nominatim Replications pod's Security Context fsGroup
  ## @param updates.podSecurityContext.seccompProfile.type Set Nominatim Replications container's Security Context seccomp profile
  ##
  podSecurityContext:
    enabled: false
    fsGroup: 101
    seccompProfile:
      type: "RuntimeDefault"
  ## Configure Container Security Context (only main container)
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  ## @param updates.containerSecurityContext.enabled Enabled Nominatim Replications containers' Security Context
  ## @param updates.containerSecurityContext.runAsUser Set Nominatim Replications container's Security Context runAsUser
  ## @param updates.containerSecurityContext.runAsNonRoot Set Nominatim Replications container's Security Context runAsNonRoot
  ## @param updates.containerSecurityContext.allowPrivilegeEscalation Set Nominatim Replications container's privilege escalation
  ## @param updates.containerSecurityContext.capabilities.drop Set Nominatim Replications container's Security Context runAsNonRoot
  ##
  containerSecurityContext:
    enabled: false
    runAsUser: 1001
    runAsNonRoot: true
    allowPrivilegeEscalation: false
    capabilities:
      drop: ["ALL"]
  ## @param updates.extraEnvVars Array with extra environment variables to add to the Nominatim container
  ## e.g:
  ## extraEnvVars:
  ##   - name: memory_limit
  ##     value: "256M"
  ##
  extraEnvVars: []
  ## @param updates.extraEnvVarsCM Name of existing ConfigMap containing extra env vars
  ##
  extraEnvVarsCM: ""
  ## @param updates.extraEnvVarsSecret Name of existing Secret containing extra env vars
  ##
  extraEnvVarsSecret: ""

  resources: { }
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #    requests:
  #      cpu: 4000m
  #      memory: 4Gi
  #    limits:
  #      cpu: 4000m
  #      memory: 4Gi

nominatimUi:
  enabled: true
  version: 3.2.1
  initContainers:
    download:
      image: curlimages/curl
  nginx:
    image: nginx:1.29.1-alpine
    imagePullPolicy: IfNotPresent

  # nginxConfiguration configures the nginx webserver that serves the UI.
  nginxConfiguration: |-
    server {
      listen 80;
    
      index search.html;
      root /nominatim/nominatim-ui/dist;
  
      # for debuging comment in this line, and the next line to your compose file:
      # error_log /var/log/nginx/error.log debug;
      # command: [nginx-debug, '-g', 'daemon off;']
  
      # n7m-api - Python endpoint
      location /api/ {
          rewrite /api/(.*) /$1 break;
          proxy_set_header Host $http_host;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $connection_upgrade;
          proxy_redirect off;
          proxy_buffering off;
          proxy_pass http://127.0.0.1:8080;
      }
  
      # n7m-api - Python health endpoint
      location /status {
          proxy_set_header Host $http_host;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
          proxy_set_header Upgrade $http_upgrade;
          proxy_set_header Connection $connection_upgrade;
          proxy_redirect off;
          proxy_buffering off;
          proxy_pass http://127.0.0.1:8080;
      }
    }
  
    map $http_upgrade $connection_upgrade {
      default upgrade;
      '' close;
    }

  configuration: |-
    Nominatim_Config.Nominatim_API_Endpoint = '/api/';





## @section Persistence Parameters
##

## Persistence Parameters
## ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/
##
flatnode:
  ## @param flatnode.enabled Enable flatnode using Persistent Volume Claims
  ##
  enabled: false
  ## @param flatnode.storageClass Persistent Volume storage class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is set, choosing the default provisioner
  ##
  storageClass:
  ## @param flatnode.accessModes [array] Persistent Volume access modes
  ##
  accessModes:
    - ReadWriteMany
  ## @param flatnode.size Persistent Volume size
  ##
  size: 100Gi
  ## @param flatnode.dataSource Custom PVC data source
  ##
  dataSource: {}
  ## @param flatnode.existingClaim The name of an existing PVC to use for persistence
  ##
  existingClaim: ""
  ## @param flatnode.selector Selector to match an existing Persistent Volume for Nominatim data PVC
  ## If set, the PVC can't have a PV dynamically provisioned for it
  ## E.g.
  ## selector:
  ##   matchLabels:
  ##     app: my-app
  ##
  selector: {}
  ## @param flatnode.annotations Persistent Volume Claim annotations
  ##
  annotations: {}
## Init containers parameters:
## volumePermissions: Change the owner and group of the persistent volume(s) mountpoint(s) to 'runAsUser:fsGroup' on each node
##
volumePermissions:
  ## @param volumePermissions.enabled Enable init container that changes the owner/group of the PV mount point to `runAsUser:fsGroup`
  ##
  enabled: false
  ## OS Shell + Utility image
  ## ref: https://hub.docker.com/r/bitnami/os-shell/tags/
  ## @param volumePermissions.image.registry [default: REGISTRY_NAME] OS Shell + Utility image registry
  ## @param volumePermissions.image.repository [default: REPOSITORY_NAME/os-shell] OS Shell + Utility image repository
  ## @skip volumePermissions.image.tag OS Shell + Utility image tag (immutable tags are recommended)
  ## @param volumePermissions.image.digest OS Shell + Utility image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param volumePermissions.image.pullPolicy OS Shell + Utility image pull policy
  ## @param volumePermissions.image.pullSecrets OS Shell + Utility image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/os-shell
    tag: 12-debian-12-r51
    digest: ""
    pullPolicy: IfNotPresent
    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []
  ## Init container's resource requests and limits
  ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
  ## @param volumePermissions.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if volumePermissions.resources is set (volumePermissions.resources is recommended for production).
  ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
  ##
  resourcesPreset: "nano"
  ## @param volumePermissions.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
  ## Example:
  ## resources:
  ##   requests:
  ##     cpu: 2
  ##     memory: 512Mi
  ##   limits:
  ##     cpu: 3
  ##     memory: 1024Mi
  ##
  resources: {}
  ## Init container' Security Context
  ## Note: the chown of the data folder is done to containerSecurityContext.runAsUser
  ## and not the below volumePermissions.containerSecurityContext.runAsUser
  ## @param volumePermissions.containerSecurityContext.seLinuxOptions [object,nullable] Set SELinux options in container
  ## @param volumePermissions.containerSecurityContext.runAsUser User ID for the init container
  ##
  containerSecurityContext:
    seLinuxOptions: {}
    runAsUser: 0

## @section Other Parameters
##

## Nominatim Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  ## @param serviceAccount.create Enable creation of ServiceAccount for Nominatim pod
  ##
  create: false
  ## @param serviceAccount.name The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the common.names.fullname template
  ##
  name: ""
  ## @param serviceAccount.automountServiceAccountToken Allows auto mount of ServiceAccountToken on the serviceAccount created
  ## Can be set to false if pods using this serviceAccount do not need to use K8s API
  ##
  automountServiceAccountToken: true
  ## @param serviceAccount.annotations Additional custom annotations for the ServiceAccount
  ##
  annotations: {}
## Nominatim Pod Disruption Budget configuration
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
## @param pdb.create Enable a Pod Disruption Budget creation
## @param pdb.minAvailable Minimum number/percentage of pods that should remain scheduled
## @param pdb.maxUnavailable Maximum number/percentage of pods that may be made unavailable
## @param pdb.pdbMatchExpressions MatchExpressions for the pdb selector. Excludes job object by default
##
pdb:
  create: false
  minAvailable: 1
  maxUnavailable: ""
  pdbMatchExpressions:
    - key: job-name
      operator: DoesNotExist

## Nominatim Autoscaling configuration
## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
## @param autoscaling.enabled Enable Horizontal POD autoscaling for Nominatim
## @param autoscaling.minReplicas Minimum number of Nominatim replicas
## @param autoscaling.maxReplicas Maximum number of Nominatim replicas
## @param autoscaling.targetCPU Target CPU utilization percentage
## @param autoscaling.targetMemory Target Memory utilization percentage
##
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 11
  targetCPU: 50
  targetMemory: 50

## @section NetworkPolicy parameters
##

## Add networkpolicies
##
networkPolicy:
  ## @param networkPolicy.enabled Enable network policies
  ## If ingress.enabled or metrics.enabled are true, configure networkPolicy.ingress and networkPolicy.metrics selectors respectively to allow communication
  ##
  enabled: false
  ## @param networkPolicy.metrics.enabled Enable network policy for metrics (prometheus)
  ## @param networkPolicy.metrics.namespaceSelector [object] Monitoring namespace selector labels. These labels will be used to identify the prometheus' namespace.
  ## @param networkPolicy.metrics.podSelector [object] Monitoring pod selector labels. These labels will be used to identify the Prometheus pods.
  ##
  metrics:
    enabled: false
    ## e.g:
    ## podSelector:
    ##   label: monitoring
    ##
    podSelector: {}
    ## e.g:
    ## namespaceSelector:
    ##   label: monitoring
    ##
    namespaceSelector: {}
  ## @param networkPolicy.ingress.enabled Enable network policy for Ingress Proxies
  ## @param networkPolicy.ingress.namespaceSelector [object] Ingress Proxy namespace selector labels. These labels will be used to identify the Ingress Proxy's namespace.
  ## @param networkPolicy.ingress.podSelector [object] Ingress Proxy pods selector labels. These labels will be used to identify the Ingress Proxy pods.
  ##
  ingress:
    enabled: false
    ## e.g:
    ## podSelector:
    ##   label: ingress
    ##
    podSelector: {}
    ## e.g:
    ## namespaceSelector:
    ##   label: ingress
    ##
    namespaceSelector: {}
  ## @param networkPolicy.ingressRules.backendOnlyAccessibleByFrontend Enable ingress rule that makes the backend (mariadb) only accessible by testlink's pods.
  ## @param networkPolicy.ingressRules.customBackendSelector [object] Backend selector labels. These labels will be used to identify the backend pods.
  ## @param networkPolicy.ingressRules.accessOnlyFrom.enabled Enable ingress rule that makes testlink only accessible from a particular origin
  ## @param networkPolicy.ingressRules.accessOnlyFrom.namespaceSelector [object] Namespace selector label that is allowed to access testlink. This label will be used to identified the allowed namespace(s).
  ## @param networkPolicy.ingressRules.accessOnlyFrom.podSelector [object] Pods selector label that is allowed to access testlink. This label will be used to identified the allowed pod(s).
  ## @param networkPolicy.ingressRules.customRules [object] Custom network policy ingress rule
  ##
  ingressRules:
    ## mariadb backend only can be accessed from testlink
    ##
    backendOnlyAccessibleByFrontend: false
    ## Additional custom backend selector
    ## e.g:
    ## customBackendSelector:
    ##   - to:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customBackendSelector: {}
    ## Allow only from the indicated:
    ##
    accessOnlyFrom:
      enabled: false
      ## e.g:
      ## podSelector:
      ##   label: access
      ##
      podSelector: {}
      ## e.g:
      ## namespaceSelector:
      ##   label: access
      ##
      namespaceSelector: {}
    ## custom ingress rules
    ## e.g:
    ## customRules:
    ##   - from:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customRules: {}
  ## @param networkPolicy.egressRules.denyConnectionsToExternal Enable egress rule that denies outgoing traffic outside the cluster, except for DNS (port 53).
  ## @param networkPolicy.egressRules.customRules [object] Custom network policy rule
  ##
  egressRules:
    # Deny connections to external. This is not compatible with an external database.
    denyConnectionsToExternal: false
    ## Additional custom egress rules
    ## e.g:
    ## customRules:
    ##   - to:
    ##       - namespaceSelector:
    ##           matchLabels:
    ##             label: example
    ##
    customRules: {}
## @section Database Parameters
##

## PostgreSQL  chart configuration
## ref: https://github.com/bitnami/charts/blob/main/bitnami/postgresql/values.yaml
##
postgresql:
  enabled: true
  image:
    repository: bitnamilegacy/postgresql


  auth:
    postgresPassword: nominatim

  primary:
    persistence:
      size: 500Gi

    ## PostgreSQL Primary resource requests and limits
    ## ref: https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/
    ## @param postgresql.primary.resourcesPreset Set container resources according to one common preset (allowed values: none, nano, micro, small, medium, large, xlarge, 2xlarge). This is ignored if primary.resources is set (primary.resources is recommended for production).
    ## More information: https://github.com/bitnami/charts/blob/main/bitnami/common/templates/_resources.tpl#L15
    ##
    resourcesPreset: "none"
    ## @param postgresql.primary.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources: { }

    extendedConfiguration: |
        shared_buffers = 2GB
        maintenance_work_mem = 10GB
        autovacuum_work_mem = 2GB
        work_mem = 50MB
        effective_cache_size = 24GB
        synchronous_commit = off
        max_wal_size = 1GB
        checkpoint_timeout = 10min
        checkpoint_completion_target = 0.9


externalDatabase:
  ## @param externalDatabase.existingSecretDsn Use an existing secret for the DSN (Data Source Name) to connect to the external PostgreSQL database
  ###
  existingSecretDsn:
  ## @param externalDatabase.existingSecretDsnKey Key in the existing secret to use for the DSN
  ##
  existingSecretDsnKey: POSTGRESQL_DSN
  ## @param externalDatabase.host External Database server host
  ##
  host: localhost
  ## @param externalDatabase.port External Database server port
  ##
  port: 5432
  ## @param externalDatabase.user External Database username
  ##
  user: nominatim
  ## @param externalDatabase.password External Database user password
  ##
  password: ""
